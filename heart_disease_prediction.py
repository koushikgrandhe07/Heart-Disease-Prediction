# -*- coding: utf-8 -*-
"""Heart Disease Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CJuV4uKDA6OdK2N09hWq9ciNX2zp3uCs
"""

#importing all necessaries libraries
import numpy as np
import pandas as pd
import warnings
warnings.filterwarnings('ignore')

#loading the dataset
heart_df = pd.read_csv("/content/heart.csv")
heart_df

#Displaying random 5 samples
heart_df.sample(5)

#Getting dataset summary
heart_df.info()

#Stastistics of the dataset
heart_df.describe()
#cholestral minimum value is zero. Chlotestoral minimum value can't be zero. So, we have to clean the data.

#Above we got only the statistics of the column where the datatype is int. Now we will get the statistics of all the columns incuding other datatypes.
heart_df.describe(include="all")

"""# Data Preprocessing"""

#Checking the null values
heart_df.isna().sum()

# Checking the duplicates
heart_df.duplicated().sum()

# Checking number of unique value in each feature
heart_df.nunique()

# We have got the column names whose data types are object and storing them in variable cat_col
cat_col = heart_df.select_dtypes(include='object').columns

"""# Converting categorical variables to Numeric


*   Sex: M=0, F=1
*   ChestPainType: ATA=0, NAP=1, ASY=2, TA=3
*   RestingECG: Normal=0, ST=1, LVH=2
*   ExerciseAngina = N=0, Y=1
*   ST_Slope: Up = 0, FLat = 1, Down = 2


"""

for col in cat_col: # Iterating through the column names in cat_col variable
  print(col) # Printing each column name
  print((heart_df[col].unique()),list(range(heart_df[col].nunique()))) #Printing total unique value in the column and fixing the
  #range depending upon the unique values in the column
  heart_df[col].replace((heart_df[col].unique()),range(heart_df[col].nunique()),inplace=True) # Here we are replacing the values to
  #range in the each column and permanently replace values in the column
  print("*"*90) #
  print()

# Displaying the dataset after changing the values
heart_df

heart_df["Cholesterol"].value_counts() # Printing the records which have 0 as the value
# So there are 172 records which are 0 and cholestoral values cannot be zero in the real world. So we have to change the values of zero
# May be a technical issue while recording

"""# Changing the 0 values in the cholesterol column using KNN Imputer"""

heart_df["Cholesterol"].replace(0,np.nan,inplace=True) # Replacing the values of 0 with nan

from sklearn.impute import KNNImputer
imputer = KNNImputer(n_neighbors=3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd.DataFrame(after_impute, columns=heart_df.columns)

heart_df['Cholesterol'].isna().sum()

"""# Doing the same for Resting Blood Pressure"""

heart_df["RestingBP"][heart_df["RestingBP"]==0]

from sklearn.impute import KNNImputer
heart_df["RestingBP"].replace(0,np.nan,inplace=True)
imputer = KNNImputer(n_neighbors=3)
after_impute = imputer.fit_transform(heart_df)
heart_df = pd.DataFrame(after_impute, columns=heart_df.columns)

heart_df["RestingBP"].unique()

heart_df['RestingBP'].isnull().sum()

"""# Change the above columns (RestingBP, Cholesterol) type into int




"""

withoutOldPeak = heart_df.columns #storing the dataset columns in the variable and oldpeak is the only column with floating data type
withoutOldPeak = withoutOldPeak.drop('Oldpeak') # dropping the old peak column from the list of columns dataframe
heart_df[withoutOldPeak] = heart_df[withoutOldPeak].astype('int32') # changing the data type of all the columns into int32 excluding Oldpeak column

heart_df.info() #checking the info of all the data types of the column.

"""# Data Visualization using plotly library"""

import plotly.express as px

# Plotting the line chart to see the correlation between the columns and heart disease
px.line(heart_df.corr()['HeartDisease'][:-1].sort_values())

"""# Age and heart disease distribution"""

px.sunburst(heart_df,path=["HeartDisease","Age"])
# From the below chart we can understand that whose age is nearly 60 they are prone to heart diesease and whose age is
#nearly they are not having any change of heart diesease

px.histogram(heart_df,x='Age',color='HeartDisease')
# Clearly we can see whose age group is from 30 to 55 they are having less chance of heart disease
# and age group from 54 and above are more prone to heart disease

"""# Percentage of Heart Disease data distribution"""

px.pie(heart_df,names='HeartDisease',title='Percentage of Heart Diesease classes distribution')
# 44.7% data present are not having disease and 55.3% are having heart disese
# We can clearly see that dataset is imbalanced

"""# Sex vs HeartDisease"""

px.histogram(heart_df,x='Sex',color='HeartDisease')
# By looking at the histogram we can say that males are having high chances of suffering from heart disease
# and females are not having more chances of suffering from heart disease

"""# Chest pain vs HeartDisease"""

px.histogram(heart_df,x='ChestPainType',color='HeartDisease')
# ATA=0, NAP=1, ASY=2, TA=3
# From below histogram if a person has has ATA no HD and if a person has NAP no HD and if a person has ASY(dangerous) there more chances of having HD
# if a person has TA there is no HD

"""# RestingBP vs HeartDisease"""

heart_df["RestingBP"].unique()
# All are unique values

px.sunburst(heart_df,path=['HeartDisease','RestingBP'])
# Patients who are having BP higher than 110 are having chances of suffering from HD
# Patients who are having BP higher than 110 are not having chances of suffering from HD
# This is beacuse if we observe correlation map the RestingBP is nearly correlated to zero
# By this we can confirm no correlation between BP and HD

"""# FastingBP and Heart Disease"""

px.histogram(heart_df,x="FastingBS",color='HeartDisease')
# In fastingBP column there are two values (0,1) if the value is 0 then FastingBP is less than 120
# if the value is 1 then fastingBP is more than 120
# if a person is having fastingBP  less than 120 there 50:50 chances of HD
# if a person is having fastingBP  greater than 120 there are more chances of HD

"""# MaxHR vs Heart Disease  """

px.sunburst(heart_df,path=['HeartDisease','MaxHR'])
# MaxHR and HD are negatively correlated

px.violin(heart_df,x='HeartDisease',y='MaxHR',color='HeartDisease')
# Accroding to the below graph if the heart rate is less then we are having more chances of suffering from HD

"""# Oldpeak vs Heart Disease"""

px.violin(heart_df,x='HeartDisease',y='Oldpeak',color='HeartDisease')
# Below graph shows if the oldpeak value is 0 then patient is not having HD
# and oldpeak value is more than 0 then patienf is suffering from heart disease

"""# ST_Slope vs Heart Disease"""

px.histogram(heart_df,x="ST_Slope",color='HeartDisease')
# if the st_slope value is 0(Down) then that patients are having less chances of HD
# if the st_slope value is 1(flat) then that patients are having more chances of HD
# if the st_slope value is 2(up) then that patients are having more chances of HD

"""# ExerciseAngina vs Heart Disease"""

px.histogram(heart_df,x="ExerciseAngina",color='HeartDisease')
# if a patient is not having EA then they cannot say wether they are having HD or not
# if patient is not having EA then we can say they are having HD

"""# Train Test Split"""

from sklearn.model_selection import train_test_split # importing library to split into train and test

x_train,x_test,y_train,y_test = train_test_split( # calling particular(train_test_split) module
    heart_df.drop('HeartDisease',axis=1), # dropping the HD column as it is the target column from heart_df
    heart_df['HeartDisease'],
    test_size=0.2, #splitting the dataset into 80% to training the model and 20% to test the model
    random_state=42, # It sets the seed for the random number generator. # This ensures that the same split will be produced every time you run the code
    stratify=heart_df['HeartDisease'] # helps to split the data of label classes into train and test with same proportionate
)
# x_train is the data which contain features of training dataset
# y_test is the data which contain featurs of testing dataset
# t_train is the data to train which contain label whether a person is having HD or not
# y_test is the data to test which contain label whether a pseron is having HD or not

"""# Model Training

# Logistic Regression(Because our problem is binary classification)
"""

from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

solver = ['lbfgs','liblinear','newton-cg','newton-cholesky','sag','saga'] # Solvers from LR documentation
best_solver = '' # Variable to store the best_solver
test_score = np.zeros(6) # Creating an array elements with 6 zeroes to store the accuracies of the 6 solvers
for i, n in enumerate(solver): # Iterating through every single solvers element and keep a track of index at every solver
  lr = LogisticRegression(solver=n).fit(x_train,y_train) # Creating the LR model and passing solver(n stores the solver) and fitting to train data
  test_score[i] = lr.score(x_test,y_test) # Evaluating the model by passing testing data and store the accuracy each solver
  if lr.score(x_test,y_test) == test_score.max(): # Checking the scores of every solver if the socre is max or not
    best_solver = n # If it is maximum then we are storing it in best_solver variable

print(best_solver)
lr = LogisticRegression(solver=best_solver) # passing the best solver into LR model
lr.fit(x_train,y_train) # passing training splits into fitness function
lr_pred = lr.predict(x_test) # Making our model to proedit it on the testing dataset
print(f'LogisticRegression Score: {accuracy_score(y_test,lr_pred)}') # checking the accuracy score by passing test data and train data

import pickle
file = open("LogisticR.pkl","wb")
pickle.dump(lr,file)

"""# Support Vector Machine (SVM)"""

from sklearn.svm import SVC # We use SVM for binary classification and our problem is binary classification
from sklearn.metrics import f1_score

kernels = {'linear':0,'poly':0,'rbf':0,'sigmoid':0} # Initialized dictionary for kernels. We use different kernels to manipulate the data
best_kernel='' # Name of the kernel which gives ur best accuracy
for i in kernels:
  svm = SVC(kernel=i) # Passing each instance
  svm.fit(x_train,y_train) # Passing training data into ftiness function
  yhat = svm.predict(x_test) # Predicting the testing data and stored in yhat variable
  kernels[i] = f1_score(y_test,yhat,average='weighted') # clculating the perfromance score by passing yhat,y_test and store it in kernels[i] variable
  if kernels[i] == max(kernels.values()): # Checking the best kernel which has given maximum value
    best_kernel = i # Storing the best kernel in the best variable

print(best_kernel)
svm = SVC(kernel=best_kernel) # Passing the best kernel into LR model
svm.fit(x_train,y_train) # Passing training splits into fitness function
svm_pred = svm.predict(x_test) # Making our model to predict it on the testing dataset
print(f'SVM f1_score kernel({best_kernel}): {f1_score(y_test,svm_pred,average="weighted")}') # checking the accuracy score by passing test data and train data

import pickle
file = open("SVM.pkl","wb")
pickle.dump(svm,file)

"""# Decision Tree"""

from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV

dtree = DecisionTreeClassifier(class_weight='balanced')
param_grid = {
    'max_depth': [3,4,5,6,7,8],
    'min_samples_split' : [2,3,4],
    'min_samples_leaf': [1,2,3,4],
    'random_state': [0,42]
}

grid_search = GridSearchCV(dtree,param_grid,cv = 5) # Passing dtree, paramgird, cross validation as parameters in GSDCV
grid_search.fit(x_train,y_train) # Passing the train splits into fitness function after that we will get best combination of param_grid
Ctree = DecisionTreeClassifier(**grid_search.best_params_,class_weight='balanced') # Inside the grid_search we can access best combination of params
Ctree.fit(x_train,y_train) # Passing training splits into fitness function
dtc_pred = Ctree.predict(x_test) # Making our model to predict it on the testing dataset
print("Decision Tree's Accuracy:",accuracy_score(y_test,dtc_pred)) # checking the accuracy score by passing test data and train data

import pickle
file = open("DT.pkl","wb")
pickle.dump(Ctree,file)

"""# Random Forest Classifier"""

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV

rfc = RandomForestClassifier()
param_grid = {
    'n_estimators': [50,100,150,500],
    'max_features': ['sqrt','log2',None],
    'max_depth': [3,6,9,19],
    'max_leaf_nodes': [3,6,9]
}

grid_search = GridSearchCV(rfc,param_grid)
grid_search.fit(x_train,y_train)
rfctree = RandomForestClassifier(**grid_search.best_params_)
rfctree.fit(x_train,y_train)
rfc_pred = rfctree.predict(x_test)
print("RandomForestClassifier's Accuracy:",accuracy_score(y_test,rfc_pred))

import pickle
file = open("RDT.pkl","wb")
pickle.dump(rfctree,file)

from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import accuracy_score

# Base model
rfc = RandomForestClassifier(random_state=42)

# Fine-tuned hyperparameter grid (recommended ranges for heart disease dataset)
param_grid = {
    'n_estimators': [100, 200, 300],              # Number of trees
    'max_depth': [4, 6, 8, None],                 # Depth of each tree
    'min_samples_split': [2, 5, 10],              # Minimum samples to split an internal node
    'min_samples_leaf': [1, 2, 4],                # Minimum samples at a leaf node
    'max_features': ['sqrt', 'log2'],             # Feature selection method at each split
    'class_weight': ['balanced', None]            # Handles class imbalance if present
}

# Grid search with 5-fold cross-validation
grid_search = GridSearchCV(rfc, param_grid, cv=5, n_jobs=-1, scoring='accuracy')
grid_search.fit(x_train, y_train)

# Train final model with best parameters
best_rf = RandomForestClassifier(**grid_search.best_params_, random_state=42)
best_rf.fit(x_train, y_train)

# Predictions and accuracy
rfc_pred = best_rf.predict(x_test)
print("RandomForestClassifier's Accuracy:", accuracy_score(y_test, rfc_pred))

# Optional: View best parameters
print("Best Hyperparameters:", grid_search.best_params_)

import pickle
file = open("GRF.pkl","wb")
pickle.dump(best_rf,file)